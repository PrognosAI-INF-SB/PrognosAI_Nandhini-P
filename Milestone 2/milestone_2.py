# -*- coding: utf-8 -*-
"""milestone 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HTNrPiTOjcyzDl3THv9lwhObOn5I7qMd
"""

# run_lstm_pipeline.py
# Milestone 1: data prep + feature engineering
# Milestone 2: LSTM build, train, evaluate
# Console-only outputs. No files written.

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# -------------------------------
# Milestone 1: Prep & Engineering
# -------------------------------

def load_and_clean(csv_path, time_col=None):
    print("Loading dataset...")
    df = pd.read_csv(csv_path)
    if time_col is None:
        candidates = [c for c in df.columns if 'time' in c.lower()]
        time_col = candidates[0] if candidates else df.columns[0]
    df = df.sort_values(by=time_col).reset_index(drop=True)
    print(f"Dataset loaded with {len(df)} rows and columns: {list(df.columns)}")

    total_missing = df.isnull().sum().sum()
    print(f"Total missing values before imputation: {total_missing}")
    if total_missing > 0:
        imputer = SimpleImputer(strategy='median')
        num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
        df[num_cols] = imputer.fit_transform(df[num_cols])
        print("Missing values imputed using median strategy.")
    else:
        print("No missing values found.")
    return df, time_col

def scale_features(df, feature_cols):
    scaler = StandardScaler()
    df[feature_cols] = scaler.fit_transform(df[feature_cols])
    print("Features scaled (StandardScaler).")
    return df

def calculate_rul(df, time_col, cap=125):
    n = len(df)
    rul = np.arange(n-1, -1, -1)
    if cap is not None and cap > 0:
        rul = np.minimum(rul, cap)
    df['RUL'] = rul
    print(f"RUL calculated with cap={cap}")
    print("Sample RUL values (first 5):", df['RUL'].head().tolist())
    return df

def generate_rolling_windows(df, feature_cols, label_col='RUL', window_size=50, stride=1):
    X, y = [], []
    n = len(df)
    print(f"Generating rolling windows with size={window_size}, stride={stride}...")
    for start in range(0, n - window_size + 1, stride):
        end = start + window_size
        X.append(df.loc[start:end-1, feature_cols].values)
        y.append(df.loc[end-1, label_col])
    X = np.asarray(X, dtype=np.float32)
    y = np.asarray(y, dtype=np.float32)
    print(f"Generated {len(X)} sequences.")
    print("\nSample Sequence Feature Window (first sequence):")
    if len(X) > 0:
        print(X[0])
        print("\nCorresponding RUL label for first sequence:", float(y[0]))
    print(f"\nX shape (samples, timesteps, features): {X.shape}")
    print(f"y shape (samples,): {y.shape}")
    return X, y

def batch_summaries(df, sensor_cols, batch_size=500):
    n = len(df)
    print(f"\nStarting batch processing with batch size={batch_size}...")
    for start in range(0, n, batch_size):
        end = min(start + batch_size, n)
        batch = df.iloc[start:end]
        print(f"\nBatch rows: {start} to {end - 1}")
        print("Mean sensor values:")
        print(batch[sensor_cols].mean())
        print(f"RUL range: {batch['RUL'].min()} to {batch['RUL'].max()}")

# -------------------------------
# Milestone 2: LSTM Training
# -------------------------------

def build_lstm(input_shape, units1=128, units2=64, dropout=0.2):
    model = Sequential([
        LSTM(units1, return_sequences=True, input_shape=input_shape),
        Dropout(dropout),
        LSTM(units2, return_sequences=False),
        Dropout(dropout),
        Dense(32, activation='relu'),
        Dense(1, activation='linear')
    ])
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss='mae',
        metrics=['mse']
    )
    return model

def train_lstm(X, y, val_split=0.2, batch_size=64, epochs=100):
    N = len(X)
    split = int(N * (1 - val_split))
    X_train, y_train = X[:split], y[:split]
    X_val, y_val = X[split:], y[split:]

    print(f"\nTrain/Val split: {X_train.shape} / {X_val.shape}")

    model = build_lstm(input_shape=(X.shape[1], X.shape[2]))

    es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[es, rlr],
        verbose=1
    )

    val_mae, val_mse = model.evaluate(X_val, y_val, verbose=0)
    print(f"\nValidation MAE: {val_mae:.4f} | Validation MSE: {val_mse:.4f}")

    preds = model.predict(X_val[:5], verbose=0).squeeze()
    print("\nPredicted RUL (first 5 val):", np.round(preds, 1).tolist())
    print("Actual RUL     (first 5 val):", np.round(y_val[:5], 1).tolist())

    return model, history

# -------------------------------
# Run everything
# -------------------------------

def main():
    # Milestone 1
    csv_path = "sensor_data.csv"
    df, time_col = load_and_clean(csv_path)
    sensor_cols = [c for c in df.columns if c != time_col and pd.api.types.is_numeric_dtype(df[c])]
    print(f"Sensor columns inferred for feature engineering: {sensor_cols}")
    df = scale_features(df, sensor_cols)
    df = calculate_rul(df, time_col=time_col, cap=125)
    X, y = generate_rolling_windows(df, feature_cols=sensor_cols, label_col='RUL', window_size=50, stride=1)
    batch_summaries(df, sensor_cols, batch_size=500)
    print("\nMilestone 1 processing complete. Proceeding to LSTM training...")

    # Guard: ensure we have data
    if len(X) < 100:
        print("Not enough sequences to train a model. Consider reducing window_size or using more data.")
        return

    # Milestone 2
    _model, _history = train_lstm(X, y, val_split=0.2, batch_size=64, epochs=100)
    print("\nTraining complete.")

if __name__ == "__main__":
    # Optional: for reproducibility
    np.random.seed(42)
    tf.random.set_seed(42)
    main()